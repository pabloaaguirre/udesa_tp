# Libraries
import datetime
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.task_group import TaskGroup
from airflow.models.connection import Connection
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator
import numpy as np
import pandas as pd
import boto3
import os
from io import StringIO

AWS_CONN_ID = "S3_default"
TEMP_DATA_PATH = "/Users/pabloaaguirre/Library/CloudStorage/GoogleDrive-pabloaaguirre@google.com/My Drive/Personal Files/UdeSA/MCD11 - ProgramacioÌn Avanzada/TP_final_programacion/udesa_tp/temp_data/"

with DAG(
    dag_id='recomendations_pipeline',
    schedule_interval=None,
    start_date=datetime.datetime(2022, 4, 1),
    catchup=False,
) as dag:

    def clean_temp_data():
        '''
        Clean existing files in the temp_data folder
        '''
        for file_name in os.listdir(TEMP_DATA_PATH):
            # construct full file path
            file = TEMP_DATA_PATH + file_name
            print(file)
            if os.path.isfile(file):
                print('Deleting file:', file)
                os.remove(file)


    def load_filter_files(bucket_name: str, files_list: list):
        '''
        Reads all files from the database in S3 and filter active advertisers
        '''
        for i in range(len(files_list)):
            source_s3 = S3Hook(AWS_CONN_ID)
            source_s3.download_file(key=files_list[i],
                                    bucket_name=bucket_name,
                                    local_path=TEMP_DATA_PATH,
                                    preserve_file_name=True,
                                    use_autogenerated_subdir=False)
        print("All files ready")

        ads_views = pd.read_csv(f"{TEMP_DATA_PATH}{files_list[0]}")
        advertiser_ids = pd.read_csv(f"{TEMP_DATA_PATH}{files_list[1]}")
        product_views = pd.read_csv(f"{TEMP_DATA_PATH}{files_list[2]}")

        ads_views = ads_views.merge(right=advertiser_ids, how="inner", on="advertiser_id")
        product_views = product_views.merge(right=advertiser_ids, how="inner", on="advertiser_id")

        ads_views.to_csv(f"{TEMP_DATA_PATH}ads_views_filtered.csv")
        product_views.to_csv(f"{TEMP_DATA_PATH}product_views_filtered.csv")

        print("filtered files ready")


    clean_temp_data = PythonOperator(
        task_id='clean_temp_data',
        python_callable=clean_temp_data
    )

    load_filter_files = PythonOperator(
        task_id='load_filter_files',
        python_callable=load_filter_files,
        op_kwargs={
            "bucket_name" : "raw-ads-database-tp-programacion-avanzada",
            "files_list" : ["ads_views.csv", "advertiser_ids.csv", "product_views.csv"]
        }
    )

    clean_temp_data >> load_filter_files

    ads_views_filtered_to_S3 = S3CreateObjectOperator(
        task_id="product_views_filtered_to_S3",
        s3_bucket="raw-ads-database-tp-programacion-avanzada",
        s3_key="airflow/ads_views_filtered.csv",
        data=f"{TEMP_DATA_PATH}ads_views_filtered.csv",
        aws_conn_id=AWS_CONN_ID,
        replace=True,
    )

    product_views_filtered_to_S3 = S3CreateObjectOperator(
        task_id="ads_views_filtered_to_S3",
        s3_bucket="raw-ads-database-tp-programacion-avanzada",
        s3_key="airflow/product_views_filtered.csv",
        data=f"{TEMP_DATA_PATH}product_views_filtered.csv",
        aws_conn_id=AWS_CONN_ID,
        replace=True,
    )

    load_filter_files >> ads_views_filtered_to_S3
    load_filter_files >> product_views_filtered_to_S3